install.packages('profvis')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
profvis({
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/OneDrive/UvA/S01P02/Machine Learning for Econometrics/Project/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
vocab
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_sample, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
dtm_train <- dtm_train[row_sums(dtm_train) > 0, ]
sort(col_sums(dtm_train), decreasing = T)[1:10]
#TF-IDF
dtm_train <- as.tbl(as.data.frame(as.matrix(dtm_train)))
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
tf <- sapply(tf_dtm_train, mean)
idf <- log2(dim(dtm_train)[1] / (col_sums(dtm_train)))
tf_idf <- tf * idf
summary(tf_idf)
})
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/OneDrive/UvA/S01P02/Machine Learning for Econometrics/Project/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_sample, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
dtm_train <- dtm_train[row_sums(dtm_train) > 0, ]
#TF-IDF
dtm_train <- as.matrix(dtm_train)
dtm_train <- as.data.frame(dtm_train)
rm(sample, vocab, pruned_vocab)
dtm_train <- as.data.frame(dtm_train)
dtm_train <- as.table(dtm_train)
dtm_train <- as.table(dtm_train)
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
dim(dtm_train)
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/OneDrive/UvA/S01P02/Machine Learning for Econometrics/Project/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
vocab
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_sample, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
dtm_train <- dtm_train[row_sums(dtm_train) > 0, ]
sort(col_sums(dtm_train), decreasing = T)[1:10]
rm(sample, vocab, pruned_vocab)
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
#TF-IDF
dtm_train <- as.matrix(dtm_train)
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
tf_dtm_train <- sapply(sweep(dtm_train, 1, row_sums(dtm_train), '/'), mean)
write_delim(dtm_train, 'tbl.csv', na = '', delim = '|')
write_csv(dtm_train, 'tbl.csv', na = '')
write.table(dtm_train, 'tbl.csv', quote = F, na = '')
install.packages('MASS')
write.matrix(dtm_train, 'tbl.csv', na = '')
library(MASS)
write.matrix(dtm_train, 'tbl.csv', na = '')
write.matrix(dtm_train, 'tbl.csv', sep = '|')
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
library(MASS)
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/Documents/Machine Learning Project/Data/ph_ads_payment_indicator.feather')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
library(MASS)
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample_size <- 0.1
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
sample %>%
group_by(user_id) %>%
summarise(count = length(user_id))
sample %>%
group_by(user_id) %>%
summarise(count = length(user_id)) %>%
filter(count > 3)
sample %>%
select(was_promoted) %>%
filter(user_id == '125')
sample %>%
filter(user_id == '125') %>%
select(user_id, was_promoted)
sample %>%
filter(user_id == '125')
x <- sample %>%
filter(user_id == '125')
View(x)
x <-
sample %>%
filter(was_promoted == 't')
View(x)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
library(MASS)
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_sample, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
dtm_train <- dtm_train[row_sums(dtm_train) > 0, ]
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
#TF-IDF
dtm_train[, 'row_sum'] <- row_sums(dtm_train)
row_sums(dtm_train)
dtm_train[, 'row_sum']
dtm_train[, 'row_sum'] <- row_sums(dtm_train)
#TF-IDF
dtm_train[, 'rowsum'] <- row_sums(dtm_train)
#TF-IDF
dtm_train[, 'rs'] <- row_sums(dtm_train)
sample_size <- 0.01
sample_size <- 1
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
sample_size <- 1
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
vocab
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
print('h')
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(feather)
library(slam)
library(text2vec)
library(stringr)
library(dplyr)
library(tokenizers)
library(tidyverse)
library(tm)
library(profvis)
library(MASS)
sample_size <- 1
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
summary(data$was_promoted)
summary(sample$was_promoted)
table(sample$was_promoted)
4115378 + 92046
sum(is.na(sample$was_promoted))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
rm(pruned_vocab)
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
rm(pruned_vocab)
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vocab
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
sample_size <- 0.01
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
sample_size <- 0.1
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
sample_size <- 1
sample_size <- 1
data <- read_feather('C:/Users/Samuel/Documents/ML_Naspers/Data/ph_ads_payment_indicator.feather')
sample <- data %>%
mutate(rnd = runif(dim(data)[1], min = 0, max = 1)) %>%
filter(rnd < sample_size)
rm(data)
prep_fun <- function(x){
#the preprocessing function, which will transfer all inputs to
#lower, replace 1 and 2 letter words, replace white spaces and digits
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', '')
x <- str_replace_all(x, '\\b\\w{1,2}\\b', '')
x <- str_replace_all(x, '\\s+', ' ')
}
tok_fun <- word_tokenizer
it_sample <- itoken(sample$description,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sample$id)
vocab <- create_vocabulary(it_sample,
stopwords = stopwords('en'))
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.7,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_sample, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
library(wordcloud)
freq = data.frame(freqterms=sort(colSums(as.matrix(dtm_train)), decreasing=TRUE))
sort(col_sums(dtm_train), decreasing = T)[1:10]
vocab
pruned_vocab
median(pruned_vocab$term_count)
median(vocab$term_count)
mean(vocab$term_count)
#TF-IDF
# dtm_train[, 'rs'] <- row_sums(dtm_train)
#
tf_dtm_train <- sweep(dtm_train, 1, row_sums(dtm_train), '/')
apply(dtm_train, 1, rowsum)
apply(dtm_train, 1, row_sums)
install.packages('dada2')
df <- data.frame(dtm_train)
df <- as.data.frame(dtm_train)
df <- as.matrix(dtm_train)
